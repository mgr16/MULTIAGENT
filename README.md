# multiagent-gpt5

Sistema multi-agente con **GPT-5** (+ mini/nano) y pipeline asÃ­ncrono para RAG, web, visiÃ³n, anÃ¡lisis numÃ©rico, crÃ­tica y sÃ­ntesis final.  
Incluye cachÃ©s (LLM/embeddings/web), FAISS, guardrails con Pydantic, evaluaciÃ³n con datasets, y A/B testing.

## ðŸ§± Estructura
```

app/
agents/           # agentes (router, planner, vision, rag, web, data, critic, summary, ...)
caching/          # caches (sqlite kv) para LLM, embeddings y web
guardrails/       # esquemas Pydantic para outputs estructurados
models/           # OpenAI (Responses API), embeddings, y modelo local (llama-cpp)
rag/              # chunking + vectorstore (FAISS)
utils/            # hashing, budget, etc.
web/              # http client con cachÃ©
eval/             # harness, mÃ©tricas y A/B
blackboard.py     # memoria compartida
scheduler.py      # orquestaciÃ³n asÃ­ncrona con cancelaciÃ³n temprana
config.py         # settings (.env)
logging_setup.py  # Loguru
main.py           # entrypoint programÃ¡tico (demo y run_query)
scripts/
run.py            # CLI sencilla
eval.py           # corre datasets y A/B
corpus/             # ejemplo de documentos para RAG
datasets/
eval/prompts.jsonl  # prompts de ejemplo para evaluaciÃ³n

````

## ðŸš€ InstalaciÃ³n
```bash
python -m venv .venv && source .venv/bin/activate
pip install -e .
cp .env.example .env
# Edita OPENAI_API_KEY y (opcional) modelos/precios
````

## â–¶ï¸ Uso rÃ¡pido

```bash
# Demo programÃ¡tico
python -m app.main

# CLI
python scripts/run.py --q "Â¿Ventajas y riesgos de usar modelos locales vs GPT-5 para anÃ¡lisis financiero?"
# con imagen (url/base64)
python scripts/run.py --q "Â¿QuÃ© muestra esta grÃ¡fica?" --image "https://.../grafico.png"
```

## âš™ï¸ ConfiguraciÃ³n de modelos (env)

En `.env` puedes elegir modelos por agente (por defecto: gpt-5 para planificaciÃ³n/datos, gpt-5-mini para sÃ­ntesis ligera, gpt-5-nano para rutas rÃ¡pidas; gpt-4o como fallback de visiÃ³n).

## ðŸ§ª Tests

```bash
pip install pytest
pytest -q
```

*Los tests â€œstubbeanâ€ las llamadas a LLM, asÃ­ que corren sin API ni red.*

## ðŸ“Š EvaluaciÃ³n y A/B

1. Edita/aÃ±ade prompts en `datasets/eval/prompts.jsonl`.
2. Corre evaluaciÃ³n base:

```bash
python scripts/eval.py --dataset datasets/eval/prompts.jsonl --runs 1
```

3. A/B con overrides (ej.: comparar summary en gpt-5-mini vs gpt-5):

```bash
python scripts/eval.py --dataset datasets/eval/prompts.jsonl --ab \
  --a '{"MODEL_SUMMARY":"gpt-5-mini"}' \
  --b '{"MODEL_SUMMARY":"gpt-5"}'
```

Resultados: `./.cache/eval_results/*.json`.

## ðŸ”’ Guardrails anti-injection

* Limpieza bÃ¡sica de HTML y exclusiÃ³n de instrucciones embebidas.
* Esquemas strict JSON para Router/Planner/Critic/Summary.
* Citaciones de RAG y URLs separadas del texto.

## ðŸ§  PolÃ­tica de escalado automÃ¡tico (incluida)

* **Ahorro**: nano/mini en routing/sÃ­ntesis; escalar a gpt-5 si `critic.conflicts>0`, contexto largo o baja confianza.
* **Complejo**: gpt-5 en Planner, Data, Summary final.

## ðŸ§¾ Observabilidad y Coste

* Logging de `usage` (tokens y coste estimado) en `bb["usage_log"]`.
* Precios por modelo configurables en `.env`.

````
